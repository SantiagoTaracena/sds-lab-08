{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Universidad del Valle de Guatemala\n",
    "# Security Data Science\n",
    "# Laboratorio 8 - Defensa de Modelos\n",
    "# Santiago Taracena Puga (20017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el vasto y dinámico campo de la ciencia de datos, la seguridad juega un papel cada vez más crucial. Con el crecimiento exponencial de los modelos de aprendizaje automático (ML) y aprendizaje profundo (DL) en diversas aplicaciones, desde la detección de fraudes hasta la conducción autónoma, la protección de estos modelos contra ataques maliciosos se ha convertido en una prioridad ineludible. Este laboratorio se sumerge en el fascinante mundo de la defensa contra ataques de evasión, inferencia, extracción y envenenamiento, abordando estrategias para proteger los modelos de ML y DL contra estos embates.\n",
    "\n",
    "La premisa fundamental radica en comprender que la defensa contra estos ataques utiliza conceptos similares a los empleados por los atacantes. Por ejemplo, los ataques de envenenamiento introducen perturbaciones deliberadas en ciertas observaciones del conjunto de datos con el objetivo de alterar el comportamiento del modelo entrenado. Una táctica de defensa contra este tipo de ataques implica la introducción de perturbaciones controladas en el conjunto de datos y el monitoreo de los efectos resultantes en las predicciones del modelo. Si las predicciones permanecen estables a pesar de estas perturbaciones, es probable que las observaciones no hayan sido envenenadas.\n",
    "\n",
    "En el caso de los ataques adversariales, donde se generan ejemplos cuidadosamente diseñados para engañar al modelo, la defensa puede implicar el entrenamiento del modelo con datos adversariales, de modo que pueda aprender a reconocer y resistir estos ataques.\n",
    "\n",
    "Para este laboratorio, se abordarán dos tipos de ataques distintos, los cuales serán implementados y posteriormente defendidos utilizando el framework Adversarial Robustness Toolbox (ART). La instalación y familiarización con esta herramienta es esencial, y se recomienda probar los ejemplos proporcionados en clase para garantizar su correcto funcionamiento.\n",
    "\n",
    "A lo largo de este Jupyter Notebook, se detallarán los pasos seguidos para llevar a cabo los ataques seleccionados, así como las estrategias de defensa implementadas y su efectividad frente a dichos ataques. El objetivo es explorar activamente cómo los conceptos teóricos se traducen en prácticas defensivas tangibles en el ámbito de la seguridad de los modelos de ML y DL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El desarrollo de este laboratorio se llevará a cabo en varias etapas bien definidas, comenzando con la selección y comprensión de los ataques a implementar, seguido por la configuración y ejecución de los mismos utilizando el framework Adversarial Robustness Toolbox (ART). Posteriormente, se abordará la fase de defensa, donde se diseñarán e implementarán estrategias para proteger los modelos de ML y DL contra estos ataques.\n",
    "\n",
    "En primer lugar, se realizará una investigación exhaustiva sobre los diferentes tipos de ataques que se pueden llevar a cabo contra modelos de ML y DL, centrándonos especialmente en los ataques de evasión, inferencia, extracción y envenenamiento. Se examinarán detalladamente los mecanismos subyacentes de cada tipo de ataque, así como los posibles impactos en la seguridad y la integridad de los modelos.\n",
    "\n",
    "Una vez seleccionados los ataques a implementar, se procederá a configurar el entorno de desarrollo, lo que incluirá la instalación del framework ART y la preparación del entorno de ejecución, que puede ser local o en la nube, dependiendo de los recursos disponibles y las preferencias del investigador. Es crucial asegurarse de que el framework ART esté correctamente instalado y funcionando antes de proceder con la implementación de los ataques y las defensas.\n",
    "\n",
    "Con el entorno listo, se pasará a la implementación de los ataques seleccionados. Esto implicará la generación de perturbaciones controladas en los datos de entrada para el modelo, con el objetivo de manipular sus predicciones de manera maliciosa. Se documentarán detalladamente los pasos realizados durante esta fase, incluyendo la generación de los ejemplos adversariales, la evaluación de su efectividad y cualquier otro aspecto relevante para comprender y replicar el proceso.\n",
    "\n",
    "Una vez completada la fase de ataque, se abordará la etapa de defensa. Aquí, se explorarán diversas estrategias para proteger el modelo contra los ataques implementados anteriormente. Esto puede implicar desde la adición de perturbaciones aleatorias a los datos de entrada para dificultar la generación de ejemplos adversariales, hasta el entrenamiento de modelos adicionales para detectar y filtrar ejemplos maliciosos.\n",
    "\n",
    "Es importante destacar que la efectividad de las estrategias de defensa dependerá en gran medida de la naturaleza y la complejidad de los ataques enfrentados. Por lo tanto, se realizarán pruebas exhaustivas para evaluar la robustez y la eficacia de las defensas implementadas, documentando cuidadosamente los resultados obtenidos y cualquier observación relevante para futuras investigaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que necesitamos realizar con el objetivo de cumplir con el desarrollo del laboratorio consiste en importar la librería `tensorflow`. Esta es la librería que nos permitirá hacer uso del modelo desarrollado para poder realizar ataques y defensas del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceso de importar la librería tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente debemos desactivar un modo que usualmente se llama \"ejecución ansiosa\" con la función `disable_eager_execution`. Este modo ocasiona un error con la ejecución del proyecto, por lo que es necesario evitar que el mismo pueda llegar a ocurrir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Desactivar ejecución ansiosa\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego debemos importar la función `load_model` para poder cargar el modelo desarrollado en el laboratorio anterior. La carga de este modelo es importante para realizar todos los pasos respectivos al presente laboratorio y su realización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceso de importar la función load_model de tensorflow\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x21384c0e410>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carga del modelo realizado\n",
    "model = load_model(\"./models/malware_classifier_model.h5\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la carga del modelo realizada, también debemos proceder a obtener el dataset de imágenes con el objetivo de lograr realizar los ataques con el apoyo del mismo. Lo primero que debemos realizar es cargar las familias de imágenes presentes en la carpeta del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase ImageDataGenerator para cargar la data\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./data/malimg_dataset/malimg_paper_dataset_imgs/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path a utilizar para cargar las imágenes\n",
    "path = \"./data/malimg_dataset/malimg_paper_dataset_imgs/\"\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9339 images belonging to 25 classes.\n"
     ]
    }
   ],
   "source": [
    "# Familias de imágenes obtenidas\n",
    "families = ImageDataGenerator().flow_from_directory(directory=path, target_size=(64, 64), batch_size=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imágenes y categorías de las familias.\n",
    "images, labels = next(families)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imágenes normalizadas\n",
    "normalized_images = images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el dataset listo, también podemos importar LabelBinarizer para transformar los labels de las imágenes y así poder utilizarlas. El proceso para realizar esta acción es el mismo proceso llevado a cabo en el laboratorio pasado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LabelBinarizer para transformar la data\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelBinarizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>LabelBinarizer()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelBinarizer()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instancia de LabelBinarizer\n",
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels codificados para cada imagen\n",
    "encoded_labels = label_binarizer.fit_transform(labels)\n",
    "encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente podemos proceder a dividir el dataset con la función `train_test_split` y obtener el dataset dividido y listo para ser utilizado para los ataques y las defensas del modelo que son necesarias para la realización del laboratorio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función train_test_split para dividir el dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(normalized_images, encoded_labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Debemos utilizar la clase `KerasClassifier` con el objetivo de cargar el modelo y poder utilizarlo con la librería ART para los procedimientos de ataque y de defensa del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Librerías necesarias para el primer ataque\n",
    "from art.estimators.classification import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "art.estimators.classification.keras.KerasClassifier(model=<keras.src.engine.sequential.Sequential object at 0x0000021384C0E410>, use_logits=False, channels_first=False, clip_values=array([0., 1.], dtype=float32), preprocessing_defences=None, postprocessing_defences=None, preprocessing=StandardisationMeanStd(mean=0.0, std=1.0, apply_fit=True, apply_predict=True), input_layer=0, output_layer=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversión del modelo a uno de clasificación de ART\n",
    "art_model = KerasClassifier(model=model, clip_values=(0, 1))\n",
    "art_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, podemos instanciar el ataque `FastGradientMethod` con el objetivo de realizar el primer ataque a defender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase FastGradientMethod para el ataque\n",
    "from art.attacks.evasion import FastGradientMethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastGradientMethod(norm=inf, eps=0.1, eps_step=0.1, targeted=False, num_random_init=0, batch_size=32, minimal=False, summary_writer=None, )"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instancia del ataque\n",
    "fgsm_attack = FastGradientMethod(estimator=art_model, eps=0.1)\n",
    "fgsm_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stara\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training_v1.py:2359: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "# Generación de ejemplos adversarios con FGSM\n",
    "X_test_adv_fgsm = fgsm_attack.generate(x=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proceso de importar la librería numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar consultas de predicción sobre los datos de entrenamiento\n",
    "predictions_training = art_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para añadir ruido gaussiano a las predicciones\n",
    "gaussian_noise = lambda predictions, sigma: predictions + np.random.normal(0, sigma, predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.29924063,  1.25140016, -0.35616057, ...,  2.61348565,\n",
       "         0.39723388,  1.25713358],\n",
       "       [-2.24233556, -0.83867007, -0.42205379, ...,  0.37238516,\n",
       "         1.36837676, -1.69303929],\n",
       "       [ 2.35845046,  1.47766767,  1.51113003, ..., -0.14228255,\n",
       "         0.71422541, -0.81162453],\n",
       "       ...,\n",
       "       [-1.66378682, -1.09034619, -0.24363825, ...,  0.64412331,\n",
       "        -1.80296432,  1.86186716],\n",
       "       [ 0.67102186, -0.19881867, -1.01395965, ..., -0.09214682,\n",
       "         0.48562811,  1.02133377],\n",
       "       [-1.10936229,  0.57537584,  0.48168241, ...,  3.26970981,\n",
       "        -0.1674564 ,  0.15887248]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Agregación de ruido a las predicciones\n",
    "noisy_predictions_training = gaussian_noise(predictions_training, 1)\n",
    "noisy_predictions_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener predicciones y generar información útil\n",
    "def get_predictions(predictions):\n",
    "\n",
    "    # Resultados de predecir cada clase\n",
    "    class_results = dict()\n",
    "\n",
    "    # Iteración sobre las predicciones realizadas\n",
    "    for prediction in predictions:\n",
    "\n",
    "        # Predicción de la clase, el valor máximo generado\n",
    "        predicted_class = np.argmax(prediction)\n",
    "\n",
    "        # Creación de la clase en el diccionario si no existe\n",
    "        if (predicted_class not in class_results):\n",
    "            class_results[predicted_class] = 1\n",
    "\n",
    "        # Agregación de un nuevo resultado\n",
    "        else:\n",
    "            class_results[predicted_class] += 1\n",
    "\n",
    "    # Retorno de los resultados obtenidos\n",
    "    return class_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones obtenidas\n",
    "class_count = get_predictions(predictions_training)\n",
    "noise_class_count = get_predictions(noisy_predictions_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase 22: 275 veces\n",
      "Clase 6: 106 veces\n",
      "Clase 2: 2097 veces\n",
      "Clase 3: 1108 veces\n",
      "Clase 19: 55 veces\n",
      "Clase 10: 273 veces\n",
      "Clase 21: 68 veces\n",
      "Clase 18: 123 veces\n",
      "Clase 15: 118 veces\n",
      "Clase 7: 152 veces\n",
      "Clase 13: 127 veces\n",
      "Clase 24: 611 veces\n",
      "Clase 14: 79 veces\n",
      "Clase 4: 149 veces\n",
      "Clase 12: 153 veces\n",
      "Clase 11: 289 veces\n",
      "Clase 23: 68 veces\n",
      "Clase 20: 104 veces\n",
      "Clase 9: 104 veces\n",
      "Clase 8: 126 veces\n",
      "Clase 1: 74 veces\n",
      "Clase 0: 85 veces\n",
      "Clase 17: 99 veces\n",
      "Clase 16: 94 veces\n"
     ]
    }
   ],
   "source": [
    "# Resultados de las predicciones normales\n",
    "for class_label, count in class_count.items():\n",
    "    print(f\"Clase {class_label}: {count} veces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase 22: 261 veces\n",
      "Clase 6: 243 veces\n",
      "Clase 21: 223 veces\n",
      "Clase 10: 262 veces\n",
      "Clase 20: 246 veces\n",
      "Clase 15: 229 veces\n",
      "Clase 3: 391 veces\n",
      "Clase 13: 267 veces\n",
      "Clase 23: 225 veces\n",
      "Clase 4: 244 veces\n",
      "Clase 14: 238 veces\n",
      "Clase 11: 300 veces\n",
      "Clase 24: 281 veces\n",
      "Clase 2: 558 veces\n",
      "Clase 17: 231 veces\n",
      "Clase 9: 258 veces\n",
      "Clase 8: 225 veces\n",
      "Clase 1: 197 veces\n",
      "Clase 12: 274 veces\n",
      "Clase 19: 244 veces\n",
      "Clase 5: 215 veces\n",
      "Clase 0: 214 veces\n",
      "Clase 7: 238 veces\n",
      "Clase 16: 219 veces\n",
      "Clase 18: 254 veces\n"
     ]
    }
   ],
   "source": [
    "# Resultados de las predicciones con ruido\n",
    "for class_label, count in noise_class_count.items():\n",
    "    print(f\"Clase {class_label}: {count} veces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La configuración del optimizador con privacidad diferencial implica ajustar parámetros clave como l2_norm_clip, noise_multiplier, num_microbatches y learning_rate. Estos parámetros controlan el nivel de ruido agregado y cómo se manejan los gradientes durante el entrenamiento del modelo. Al compilar el modelo con este optimizador, se garantiza que todas las actualizaciones de los parámetros durante el entrenamiento se realicen de manera privada, preservando así la confidencialidad de los datos utilizados.\n",
    "\n",
    "En términos de efectividad, al analizar las predicciones originales, se observa una alta variabilidad en la frecuencia de las clases, con algunas clases siendo mucho más frecuentes que otras. Sin embargo, al agregar ruido, estas frecuencias se vuelven más uniformes, lo que dificulta que un atacante infiera información específica sobre las clases predichas. Esto se evidencia en la reducción de la variabilidad en las predicciones, lo que sugiere una mejora en la privacidad del modelo al hacer más difícil para un atacante discernir patrones o características específicas en las predicciones. El ruido introducido ha logrado ocultar las características precisas de las predicciones originales, lo que refuerza la protección de la privacidad de los datos utilizados en el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from art.estimators.classification import TensorFlowV2Classifier\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar y cargar el conjunto de datos MNIST\n",
    "mnist_train, mnist_info = tfds.load(name=\"mnist\", split=\"train\", with_info=True)\n",
    "mnist_test = tfds.load(name=\"mnist\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesamiento de datos\n",
    "def preprocess(dataset):\n",
    "    image = tf.image.convert_image_dtype(dataset[\"image\"], dtype=tf.float32)\n",
    "    image = tf.reshape(image, [-1])\n",
    "    return image, dataset[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = mnist_train.map(preprocess).batch(32)\n",
    "mnist_test = mnist_test.map(preprocess).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo TensorFlow\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear el clasificador TensorFlow\n",
    "classifier = TensorFlowV2Classifier(model=model, nb_classes=10, input_shape=(784,), loss_object=tf.keras.losses.SparseCategoricalCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1875 steps\n",
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 1ms/step - batch: 937.0000 - size: 1.0000 - loss: 0.2604 - accuracy: 0.9253\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 2s 868us/step - batch: 937.0000 - size: 1.0000 - loss: 0.1119 - accuracy: 0.9676\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 2s 827us/step - batch: 937.0000 - size: 1.0000 - loss: 0.0762 - accuracy: 0.9778\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 889us/step - batch: 937.0000 - size: 1.0000 - loss: 0.0563 - accuracy: 0.9838\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 843us/step - batch: 937.0000 - size: 1.0000 - loss: 0.0420 - accuracy: 0.9880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2143c3f5450>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "model.fit(mnist_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar ejemplos adversariales con FGSM\n",
    "fgsm_attack = FastGradientMethod(classifier, eps=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión del modelo en ejemplos adversariales generados por FGSM: 97.36%\n"
     ]
    }
   ],
   "source": [
    "# Evaluar la robustez del modelo en los ejemplos adversariales\n",
    "predictions = np.argmax(classifier.predict(adv_dataset), axis=1)\n",
    "accuracy = np.sum(predictions == mnist_test_label) / len(mnist_test_label)\n",
    "print(\"Precisión del modelo en ejemplos adversariales generados por FGSM: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se procede a diseñar un modelo de red neuronal convolucional (CNN) utilizando la biblioteca Keras de TensorFlow. Este modelo se configura de manera secuencial, lo que significa que las capas se apilan una sobre otra en orden. La estructura de la CNN incluye varias capas convolucionales, que detectan características específicas en las imágenes de entrada, seguidas de capas de agrupación máxima que reducen la dimensionalidad de las características. Luego, las capas de aplanamiento convierten la salida de las capas convolucionales en un vector unidimensional para que pueda ser procesado por capas densas (totalmente conectadas), que aprenden a clasificar las características extraídas. Finalmente, la capa de salida utiliza una función de activación softmax para producir probabilidades de pertenencia a cada clase.\n",
    "\n",
    "Una vez definida la arquitectura del modelo, se procede a compilarlo. La compilación implica la configuración de parámetros adicionales necesarios para el proceso de entrenamiento, como el optimizador y la función de pérdida. En este caso, se utiliza el optimizador Adam, que es una variante del descenso de gradiente estocástico (SGD) conocida por su eficiencia y adaptabilidad. Además, se selecciona la función de pérdida de entropía cruzada categórica, adecuada para problemas de clasificación con más de dos clases.\n",
    "\n",
    "Con el modelo compilado, se prepara para su uso con las herramientas de defensa contra ataques adversarios proporcionadas por la biblioteca Adversarial Robustness Toolbox (ART). Esto implica envolver el modelo con la clase TensorFlowV2Classifier de ART, que proporciona una interfaz compatible con las funcionalidades de defensa de la biblioteca.\n",
    "\n",
    "Se define una función llamada adversarial_training que implementa el entrenamiento adversarial. Esta función recibe como entrada el modelo, los datos de entrenamiento (imágenes y etiquetas), una lista de valores de epsilon que controlan la magnitud del perturbación en los ejemplos adversarios, y un parámetro adicional llamado ratio. En cada iteración sobre los valores de epsilon, la función genera ejemplos adversarios utilizando el ataque de Gradiente Proyectado (PGD) y los combina con los datos originales de entrenamiento. Este proceso de combinación ajusta el tamaño de los datos según el ratio especificado y los aleatoriza antes de utilizarlos para entrenar el modelo durante una época.\n",
    "\n",
    "Una vez definida la función de entrenamiento adversarial, se procede a aplicarla al modelo utilizando los datos de entrenamiento originales (X_train, y_train) y una lista de valores de epsilon seleccionados previamente. Este paso es crucial para mejorar la robustez del modelo frente a posibles ataques adversarios, ya que le permite aprender a reconocer y resistir las perturbaciones introducidas por los adversarios.\n",
    "\n",
    "Posteriormente, se utiliza el ataque de Gradiente Proyectado para generar ejemplos adversarios basados en los datos de prueba (X_test). Estos ejemplos adversarios se utilizan para evaluar la efectividad de la defensa implementada mediante el entrenamiento adversarial.\n",
    "\n",
    "Para evaluar la efectividad de la defensa, se comparan las métricas de precisión del modelo antes y después de aplicar el entrenamiento adversarial. Antes de la defensa, el modelo muestra una alta precisión en los datos originales, pero su rendimiento en ejemplos adversarios generados con el ataque PGD es significativamente bajo. Después de aplicar el entrenamiento adversarial y evaluar el modelo en ejemplos adversarios generados con un valor de epsilon específico, se observa una mejora modesta pero significativa en la precisión del modelo en estos casos. Esto indica que el entrenamiento adversarial ha mejorado la capacidad del modelo para defenderse contra ataques adversarios en comparación con su configuración anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En conclusión, este laboratorio ha sido una exploración valiosa en la mejora de la robustez de los modelos de aprendizaje automático frente a ataques adversarios. A través de la implementación del entrenamiento adversarial y la evaluación de su efectividad utilizando la biblioteca Adversarial Robustness Toolbox (ART), hemos podido observar cómo los modelos pueden fortalecerse contra posibles amenazas mediante el aprendizaje de características robustas y la adaptación a perturbaciones en los datos de entrada.\n",
    "\n",
    "Al diseñar y entrenar una red neuronal convolucional (CNN) para clasificar imágenes, hemos comprendido la importancia de la seguridad en el despliegue de sistemas de inteligencia artificial, especialmente en entornos críticos donde la confiabilidad y la integridad de los resultados son fundamentales. El análisis de la precisión del modelo antes y después de la aplicación del entrenamiento adversarial revela un aumento en la capacidad de defensa del modelo, incluso ante perturbaciones significativas en los datos de entrada.\n",
    "\n",
    "Este laboratorio también ha destacado la necesidad continua de investigación y desarrollo en el campo de la seguridad de la inteligencia artificial. Si bien el entrenamiento adversarial muestra promesas como una estrategia efectiva para mejorar la robustez de los modelos, aún existen desafíos y limitaciones que deben abordarse, como la búsqueda de técnicas más sofisticadas de generación de ejemplos adversarios y la comprensión de cómo los modelos pueden adaptarse dinámicamente a nuevas amenazas.\n",
    "\n",
    "En última instancia, este laboratorio no solo ha proporcionado una visión práctica de las técnicas de defensa contra ataques adversarios en el aprendizaje automático, sino que también ha fomentado una mayor conciencia sobre la importancia de la seguridad y la confiabilidad en el desarrollo de sistemas de inteligencia artificial. Al continuar investigando y aplicando medidas de seguridad como el entrenamiento adversarial, podemos avanzar hacia un futuro donde los modelos de aprendizaje automático sean más robustos, confiables y seguros en una amplia gama de aplicaciones y escenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
